= Comparing Golang Concurrent Programming with OpenMP in C: Block-wise Matrix-matrix Multiplication Benchmark
John Lyons(c) 2022
Version 0.1, 09.01.2022
:sectnums:
:toc:
:toclevels: 4                                                       
:toc-title: Overview                                    
                                                
:description: Matrix-matrix multiplication with OpenMP in C vs. Golang
:keywords: matrix multiplication, Golang, C, OpenMP, concurrency, parallelism                             
:imagesdir: ./figures                                                 

== Introduction

The C programming language exists since 1972 and was one of the most often used programming languages in the past 50 years. It is imperative, statically typed and compiles into native cpu instructions. By design it allows low-level access to hardware and memory and thus various operating systems or IoT Devices are programmed with C.

During the past years multi-core programming became more popular because new cpus tend to have more cores than older ones. The image below shows that, starting from around 2005, the number of physcical cores is constantly rising whereas the increase in frequency stops.

image::parallel-trend.png[Concurrency, 400, align="center"]

As such programs extensively profit from concurrent programming. Because C is relatively old it does not come with concurrency in mind: One has to use the operating systems' features in order to implement parallel logic. While this is not bad by itself there is still the need of managing multiply threads, which can be annoying at times or can lead to mistakes.

OpenMP is a library which was first released in 1997 for the Fortran programming language. The C version of OpenMP was released one year later. It comes in mind of parallelizing classic __loops__ where the amount of iterations is already known in the beginning. Implementing OpenMP is theoretically fairly easy but comes with many traps as we will see later.

On the other hand __Golang__ is a new language released in 2009 by Rob Pike et. al.. It aims to replace C/C++ in general purpose settings and as such brings some improvements and changes compared to C. For example it contains a garbage collector which frees unused memory similar to java. It comes with concurrency in mind by providing such called __goroutines__ which allow easy implementation of parallel programming paradigms. On the other hand Golang inherits some of the core concepts of C such as pointers.

When it comes to comparing concurrent programming in both Golang and C the matrix-matrix multiplication is a perfect benchmark because it almost always allows for spawning many parallel threads or thread-like constructs.

These days matrix multiplications are very common, for example in game rendering or machine learning scenarios. Speaking of machine learning neural models are constantly gaining more and more popularity. Those models have can have a large amount of parameters and as such training those models is computationally expensive. Training data are being loaded into matrices and those are multiplied with matrices representing the neural model's parameters across all layers. 

Solutions for fast matrix-matrix multiplication already exist. For example there is Basic Linear Algebra Subprograms (BLAS) which can be used in C in order to make use of an efficient matrix-matrix multiplication. Other than that there is __nvidia cuda__ or __amd rocm__ which allow relocating concurrent workloads on gpus which are highly optimized for such tasks.

As such the proposed implementation does not primarily focus on outperforming those solutions. Instead the main goal is to compare the benefits gained by parallelizing a vanilla or blocked matrix-matrix multiplication and showing pros and cons in both C/OpenMP and Go.

Because both implementations would extensively make profit of easy optimizations the memory layout and some of the core ideas of matrix-matrix multiplication are explained in the following chapter.

IMPORTANT NOTES: Iterate over i -> if row count low then useless
Use accumulator for "k-loop" or too many memory access -> slow

== Matrix-multiplication basics

The figure below depicts a basic matrix-matrix multiplication.

image::mm.svg[static,400,align="center"]

Let us denote the left hand matrix as A, the right hand matrix as B and the result matrix as C. Then, in order to multiply both matrices together, the column count of A has to be equal to the row count of B. This is the case because the rows of A are multiplied with the columns of B element-wise, as the green arrow in the above picture shows. The result is stored in the destination matrix C using the row index of A and column index of B. 

Therefore each combination of a row of A and a column of B results in a unique index for matrix C. Because of that there is no race condition when constructing a correct loop order and one can make use of parallelization.

However care has to be taken when choosing the axis to parallelize. For example if matrix A had 1000000 rows and 2 columns it would make no sense to parallelize the column computations of A as most modern computers are capable to launch more than 2 real parallel threads. On the other hand special treatment is needed when launching more threads than physically possible. This is often realized by a software threads and a scheduler. We will see later that launching a large amount of goroutines is relatively efficient in Golang.

Other than that there exists the approach of partitioning matrices into blocks before performing multiplication. The figure below shows how this can look like.

image::mm-block.svg[static, 400, align="center"]

The matrix is split into chunks of desired __block size__ whenever possible. The constraints for dimensions of the resulting block-matrices are the same as the dimension constraints when performing a vanilla matrix-matrix multiplication. As far as the multiplication with rows of A and columns of B is concerned the rules stay the same. At the end of the matrix-matrix multiplication with a row of block-matrix A and a column of block-matrix B the results are added together. This time a matrix-matrix addition is performed to sum up the results into a result sub-matrix. It is advisable not to store the temporary results of sub-matrix multiplications but rather to add on top of the existing values in C. This will be later showed in the example code.

Summing it up the major differences are:

* The element-wise multiplication of sub-matrices is now a classic matrix-matrix multiplication
* Each sub-matrix multiplication has to take existing values in target matrix into account in order to bypass the matrix-matrix addition requirement

As far as performance is concerned there is a simple yet effective way to speed up the multiplication of matrices which requires to transpose matrix B. This way the memory layout of matrix B changes so that the access of a column of B is more cache friendly. For the sake of simplicity and because the project focuses on concurrency benchmarks rather than raw performance this optimization step was skipped.

The next chapter will discuss the C implementation which uses the OpenMP library.

== Implementation in C and OpenMP
// cmake, c99 vs c89, compiler warns important, opt important, race conditions not possible in our impl
// autosync/wait for all threads to finish
// testing with c++ framework, needs a lot of setup for coverage etc
// All tested algorithm modes (listing?)

// Chap. Implementation in Go
// easy out of the box
// dont need to control amount of goroutines because fast
// wg.Sync needed 
// for sake of simplicity only quadratic matrices, but same applies to other (as seen in c project)
// testing relatively easy and nice

==  Benchmark results

// Talk about default used block size, mat dimensions
[cols=5*,options=header]
|===
|*Algorithm Type*
|*Golang on Windows PC*
|*C with OpenMP on Windows PC*
|*Golang on Linux Laptop*
|*C with OpenMP on Linux Laptop*

|*Vanilla*
|150071ms
|72420ms
|41753ms
|64560ms

|*Vanilla parallel*
|27578ms
|18920ms
|4354ms
|13800ms

|*Blocked*
|32884ms
|-
|27462ms
|-

|*Blocked parallel*
|6676ms
|3530ms
|5228ms
|3770ms

|*Prepared blocked*
|-
|16720ms
|-
|15940ms

|*Prepared blocked parallel*
|-
|2090ms
|-
|2450ms
|===

////
BENCHMARK RESULTS:
LAPTOP:
C with OMP, Release
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "64.56" ms
Starting calc with parallel vanilla omp algorithm:
Took "13.80" ms
Starting calc with prepared blocked algorithm:
Took "15.94" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "2.45" ms
Starting calc with parallel inline blocked omp algorithm:
Took "3.77" ms

Go with goroutine:
Creating matrices A and B with col/row count = 3000 and max init value = 10000
Using block size = 50 for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "41753" ms
Starting calc with parallel vanilla extern gofunc algorithm:
Took "4354" ms
Starting calc with blocked algorithm:
Took "27462" ms
Starting calc with parallel blocked extern gofunc algorithm:
Took "5228" ms

PC:
C with OMP, Release
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "72.42" ms
Starting calc with parallel vanilla omp algorithm:
Took "18.92" ms
Starting calc with prepared blocked algorithm:
Took "16.72" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "2.09" ms
Starting calc with parallel inline blocked omp algorithm:
Took "3.53" ms

Go with goroutine:
Creating matrices A and B with col/row count = 3000 and max init value = 10000
Using block size = 50 for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "150071" ms
Starting calc with parallel vanilla extern gofunc algorithm:
Took "27578" ms
Starting calc with blocked algorithm:
Took "32884" ms
Starting calc with parallel blocked extern gofunc algorithm:
Took "6676" ms

In C OPTIMIZATION is very important! Does it automatically arrange memory of B so it acts like it was transposed?
////