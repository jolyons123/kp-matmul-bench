= Comparing Golang conrurrent programming with OpenMP in C: Block-wise matrix-matrix multiplication benchmark
John Lyons(c) 2022
Version 0.1, 09.01.2022
:sectnums:
:toc:
:toclevels: 4                                                       
:toc-title: Overview                                    
                                                
:description: Matrix-matrix multiplication with OpenMP in C vs. Golang
:keywords: matrix multiplication, Golang, C, OpenMP, concurrency, parallelism                             
:imagesdir: ./figures                                                 

== Introduction

The C programming language exists since 1972 and is in one of the most often used programming languages in the past 50 years. It is imperative, statically typed and compiles into native cpu instructions. By design it allows low-level access to hardware and memory and thus various operating systems or IoT Devices are programmed with C.

During the past years multi-core programming became more popular because new cpus tend to have more cores than older ones. As such programs extensively profit from concurrent programming. Because C is relatively old it does not come with concurrency in mind: One has to use the operating systems' features in order to implement parallel logic. While this is not bad by itself there is still the need of managing multiply threads, which can be annoying at times or can lead to mistakes.

OpenMP is a library which was first released in 1997 for the Fortran programming language. The C version of OpenMP was released one year later. It comes in mind of parallelizing classic __loops__ where the amount of iterations is already known in the beginning. Implementing OpenMP is theoretically fairly easy but comes with many traps as we will see later.

On the other hand __Golang__ is a new language released in 2009 by Rob Pike et. al. ......

These days matrix multiplications are very common, for example in game rendering or machine learning scenarios. Speaking of machine learning neural models are constantly gaining more and more popularity. Those models have can have a large amount of parameters and as such training those models is computationally expensive. Training data are being loaded into matrices and those are multiplied with matrices representing the neural model's parameters across all layers. 

Solutions for fast matrix-matrix multiplication already exist. For example there is Basic Linear Algebra Subprograms (BLAS) which can be used in C in order to make use of an efficient matrix-matrix multiplication. Other than that there is __nvidia cuda__ or __amd rocm__ which allow relocating concurrent workloads on gpus which are highly optimized for such tasks.

Because both the Go as well as the OpenMP implementation heavily rely on the memory layout and some of the core ideas of matrix-matrix multiplication the following chapter deals with some of the basics.

IMPORTANT NOTES: Iterate over i -> if row count low then useless
Use accumulator for "k-loop" or too many memory access -> slow
== Matrix-multiplication basics

Fig. XYZ depicts a basic matrix-matrix multiplication..

==  Benchmarks
LAPTOP:
C with OMP, Debug
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "145.48" ms
Starting calc with parallel vanilla omp algorithm:
Took "23.27" ms
Starting calc with prepared blocked algorithm:
Took "98.49" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "17.44" ms
Starting calc with parallel inline blocked omp algorithm:
Took "47.85" ms

C with OMP, Release
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "64.56" ms
Starting calc with parallel vanilla omp algorithm:
Took "13.80" ms
Starting calc with prepared blocked algorithm:
Took "15.94" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "2.45" ms
Starting calc with parallel inline blocked omp algorithm:
Took "3.77" ms

Go with goroutine:
Creating matrices A and B with col/row count = 3000 and max init value = 10000
Using block size = 50 for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "41753" ms
Starting calc with parallel vanilla extern gofunc algorithm:
Took "4354" ms
Starting calc with blocked algorithm:
Took "27462" ms
Starting calc with parallel blocked extern gofunc algorithm:
Took "5228" ms

PC:
C with OMP, Release
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "72.42" ms
Starting calc with parallel vanilla omp algorithm:
Took "18.92" ms
Starting calc with prepared blocked algorithm:
Took "16.72" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "2.09" ms
Starting calc with parallel inline blocked omp algorithm:
Took "3.53" ms

In C OPTIMIZATION is very important! Does it automatically arrange memory of B so it acts like it was transposed?
