= Comparing Golang Concurrent Programming with OpenMP in C: Block-wise Matrix-matrix Multiplication Benchmark
John Lyons(c) 2022
Version 0.1, 09.01.2022
:sectnums:
:toc:
:toclevels: 4                                                       
:toc-title: Overview                                    
                                                
:description: Matrix-matrix multiplication with OpenMP in C vs. Golang
:keywords: matrix multiplication, Golang, C, OpenMP, concurrency, parallelism                             
:imagesdir: ./figures                                                 

== Introduction

The C programming language exists since 1972 and was one of the most often used programming languages in the past 50 years. It is imperative, statically typed and compiles into native cpu instructions. By design it allows low-level access to hardware and memory and thus various operating systems or IoT Devices are programmed with C.

During the past years multi-core programming became more popular because new cpus tend to have more cores than older ones. The image below shows that, starting from around 2005, the number of physcical cores is constantly rising whereas the increase in frequency stops.

image::parallel-trend.png[Concurrency, 400, align="center"]

As such programs extensively profit from concurrent programming. Because C is relatively old it does not come with concurrency in mind: One has to use the operating systems' features in order to implement parallel logic. While this is not bad by itself there is still the need of managing multiply threads, which can be annoying at times or can lead to mistakes.

OpenMP is a library which was first released in 1997 for the Fortran programming language. The C version of OpenMP was released one year later. It comes in mind of parallelizing classic __loops__ where the amount of iterations is already known in the beginning. Implementing OpenMP is theoretically fairly easy but comes with many traps as we will see later.

On the other hand __Golang__ is a new language released in 2009 by Rob Pike et. al.. It aims to replace C/C++ in general purpose settings and as such brings some improvements and changes compared to C. For example it contains a garbage collector which frees unused memory similar to java. It comes with concurrency in mind by providing such called __goroutines__ which allow easy implementation of parallel programming paradigms. On the other hand Golang inherits some of the core concepts of C such as pointers.

When it comes to comparing concurrent programming in both Golang and C the matrix-matrix multiplication is a perfect benchmark because it almost always allows for spawning many parallel threads or thread-like constructs.

These days matrix multiplications are very common, for example in game rendering or machine learning scenarios. Speaking of machine learning neural models are constantly gaining more and more popularity. Those models have can have a large amount of parameters and as such training those models is computationally expensive. Training data are being loaded into matrices and those are multiplied with matrices representing the neural model's parameters across all layers. 

Solutions for fast matrix-matrix multiplication already exist. For example there is Basic Linear Algebra Subprograms (BLAS) which can be used in C in order to make use of an efficient matrix-matrix multiplication. Other than that there is __nvidia cuda__ or __amd rocm__ which allow relocating concurrent workloads on gpus which are highly optimized for such tasks.

As such the proposed implementation does not primarily focus on outperforming those solutions. Instead the main goal is to compare the benefits gained by parallelizing a vanilla or blocked matrix-matrix multiplication and showing pros and cons in both C/OpenMP and Go.

Because both implementations would extensively make profit of easy optimizations the memory layout and some of the core ideas of matrix-matrix multiplication are explained in the following chapter.

IMPORTANT NOTES: Iterate over i -> if row count low then useless
Use accumulator for "k-loop" or too many memory access -> slow

== Matrix-multiplication basics

The figure below depicts a basic matrix-matrix multiplication.

image::mm.svg[static,300,align="center"]

Let us denote the left hand matrix as A and the right hand matrix as B. Then, in order to multiply both matrices together, the column count of A has to be equal to the row count of B


// BLOCKS explain
// All tested algorithm modes (listing?)

// Chap. Implementation in C and OpenMP
// cmake, c99 vs c89, compiler warns important, opt important, race conditions not possible in our impl
// autosync/wait for all threads to finish
// testing with c++ framework, needs a lot of setup for coverage etc

// Chap. Implementation in Go
// easy out of the box
// dont need to control amount of goroutines because fast
// wg.Sync needed 
// for sake of simplicity only quadratic matrices, but same applies to other (as seen in c project)
// testing relatively easy and nice

==  Benchmark results

// Talk about default used block size, mat dimensions
[cols=5*,options=header]
|===
|*Algorithm Type*
|*Golang on Windows PC*
|*C with OpenMP on Windows PC*
|*Golang on Linux Laptop*
|*C with OpenMP on Linux Laptop*

|*Vanilla*
|150071ms
|72420ms
|41753ms
|64560ms

|*Vanilla parallel*
|27578ms
|18920ms
|4354ms
|13800ms

|*Blocked*
|32884ms
|-
|27462ms
|-

|*Blocked parallel*
|6676ms
|3530ms
|5228ms
|3770ms

|*Prepared blocked*
|-
|16720ms
|-
|15940ms

|*Prepared blocked parallel*
|-
|2090ms
|-
|2450ms
|===

////
BENCHMARK RESULTS:
LAPTOP:
C with OMP, Release
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "64.56" ms
Starting calc with parallel vanilla omp algorithm:
Took "13.80" ms
Starting calc with prepared blocked algorithm:
Took "15.94" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "2.45" ms
Starting calc with parallel inline blocked omp algorithm:
Took "3.77" ms

Go with goroutine:
Creating matrices A and B with col/row count = 3000 and max init value = 10000
Using block size = 50 for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "41753" ms
Starting calc with parallel vanilla extern gofunc algorithm:
Took "4354" ms
Starting calc with blocked algorithm:
Took "27462" ms
Starting calc with parallel blocked extern gofunc algorithm:
Took "5228" ms

PC:
C with OMP, Release
Creating matrix A with rows = 3000, cols = 3000 and B with rows = 3000, cols = 3000 and max init value = 10000
Using block size = (50, 50) for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "72.42" ms
Starting calc with parallel vanilla omp algorithm:
Took "18.92" ms
Starting calc with prepared blocked algorithm:
Took "16.72" ms
Starting calc with parallel prepared blocked omp algorithm:
Took "2.09" ms
Starting calc with parallel inline blocked omp algorithm:
Took "3.53" ms

Go with goroutine:
Creating matrices A and B with col/row count = 3000 and max init value = 10000
Using block size = 50 for blocked mm algorithm
Starting calc with vanilla algorithm:
Took "150071" ms
Starting calc with parallel vanilla extern gofunc algorithm:
Took "27578" ms
Starting calc with blocked algorithm:
Took "32884" ms
Starting calc with parallel blocked extern gofunc algorithm:
Took "6676" ms

In C OPTIMIZATION is very important! Does it automatically arrange memory of B so it acts like it was transposed?
////